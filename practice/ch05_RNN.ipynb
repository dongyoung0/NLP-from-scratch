{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 기존 feed forward 방식에서 시계열 데이터의 성질(패턴)을 충분히 학습하기 위해 RNN(Recurrent Neural Network) 도입."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 확률과 언어 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- CBOW 모델의 목적 : 맥락(context)으로부터 타깃(target)을 정확하게 추측하는 것\n",
    "- 언어 모델(Language Model) : 단어 나열에 확률을 부여함.\n",
    "    - 단어의 sequence가 일어날 가능성이 어느정도인지 확률로 평가.(얼마나 자연스러운 단어 순서인지)\n",
    "    - t-1번째 까지의 단어가 나왔을 때, t번째 단어가 나올 확률을 계산하여(조건부 언어 모델, Conditional Language Model) 동시 확률을 구함\n",
    "    - Markov Chain을 통해 확률 근사\n",
    "<br/> <img src='../figs/e%205-8.png'> <br/>\n",
    "\n",
    "- CBOW(Continuous Bag-Of-Words)는 '가방안에 들어 있는 단어들'이라는 이름에서 알 수 있듯이 가방 속의 단어의 순서는 무시함.\n",
    "- 위 언어모델 확률 계산에서, 맥락의 크기를 고정하여(위의 경우 2) 확률을 근사했는데, CBOW의 경우 맥락의 크기 밖의 정보를 무시하는 문제가 발생함.\n",
    "<br/> <img src='../figs/fig%205-5.png'> <br/>\n",
    "- CBOW는 왼쪽 그림과 같이 단어 벡터들이 더해지면서 단어의 순서가 무시됨\n",
    "- 신경 확률론적 언어모델(Neural Probablistic Language Model)에서는 오른쪽 그림과 같이 단어를 더하는 대신 연결(concatenate)하는 방식을 사용\n",
    "- 이 때 W의 parameter가 늘어나는 문제를 RNN을 사용하여 해결!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 앞서 만들었던 단어의 분산 표현($x_t$)을 순서대로 RNN layer에 입력\n",
    "<br/> <img src='../figs/fig%205-8.png'> <br/>\n",
    "- 각 hidden layer에서 두 input을 바탕으로 output 계산\n",
    "<br/> <img src='../figs/e%205-9.png'> <br/>\n",
    "- 이때 $t$번째, '시각 $t$'에서의 출력 $h_t$를 은닉 상태($hidden~state$)라고 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPTT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BPTT(Backpropagation Through Time) : 시간 방향으로 펼친 신경망의 오차역전파법\n",
    "<br/> <img src='../figs/fig%205-10.png'> <br/>\n",
    "- 그러나 BPTT을 이용할 때, 계산량 뿐만 아니라 RNN layer 중간 데이터를 메모리에 유지해 두어야 하기 때문에 메모리 사용량도 증가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncated BPTT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 길어진 신경망 연결을 적당한 길이로 끊어서 계산량을 줄임\n",
    "- layer가 많아지면서 기울기 값이 점점 작아져서 0에 가까워지는 문제도 해결\n",
    "- forward propagation은 유지하고 backward의 연결만 적당한 길이로 끊어야 함\n",
    "<br/> <img src='../figs/fig%205-11.png'> <br/>\n",
    "- RNN 학습시에 forward propagation를 순서대로 수행하고 그 다음 backward propagaion을 수행함\n",
    "- 그러므로 기존 신경망에서 미니배치 학습을 수행할 떄 데이터를 무작위로 선택했던 것과 다르게 데이터를 순서대로 입력해야 함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 랜덤하게 미니 배치를 선택했던 것과 다르게 데이터를 순서대로 입력해야함.\n",
    "- 각 미니배치마다 시작 위치를 batch-size만큼 옮겨줘야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 길이가 T인 시계열 데이터를 받아서, 각 시각의 hidden state T개를 출력.\n",
    "- 이때 입력($xs$), 출력($hs$)를 묶은 layer를 Time RNN layer라고 함\n",
    "<br/> <img src='../figs/fig%205-17.png'> <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-23T07:00:44.361193Z",
     "start_time": "2021-10-23T07:00:44.340012Z"
    }
   },
   "outputs": [],
   "source": [
    "# RNN layer\n",
    "class RNN:\n",
    "    def __init__(self, Wx, Wh, b):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, x, h_prev):\n",
    "        Wx, Wh, b = self.params\n",
    "        t = np.dot(h_prev, Wh) + np.dot(x, Wx) + b\n",
    "        h_next = np.tanh(t)\n",
    "        \n",
    "        self.cache = (x, h_prev, h_next)\n",
    "        \n",
    "        return h_next\n",
    "    \n",
    "    def backward(self, dh_next):\n",
    "        Wx, Wh, b = self.params\n",
    "        x, h_prev, h_next = self.cache\n",
    "        \n",
    "        dt = dh_next * (1 - h_next ** 2)\n",
    "        db = np.sum(dt, axis=0)\n",
    "        dWh = np.dot(h_prev.T, dt)\n",
    "        dh_prev = np.dot(dt, Wh.T)\n",
    "        dWx = np.dot(x.T, dt)\n",
    "        dx = np.dot(dt, Wx.T)\n",
    "        \n",
    "        self.grads[0][...] = dWx\n",
    "        self.grads[1][...] = dWh\n",
    "        self.grads[2][...] = db\n",
    "        \n",
    "        return dx, dh_prev\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- stateful(True/False) : Time RNN이 hidden state를 유지할지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-23T07:01:52.337835Z",
     "start_time": "2021-10-23T07:01:52.304307Z"
    }
   },
   "outputs": [],
   "source": [
    "class TimeRNN:\n",
    "    def __init__(self, Wx, Wh, b, stateful=False):\n",
    "        self.params = [Wx, Wh, b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.layers = None\n",
    "\n",
    "        self.h, self.dh = None, None\n",
    "        self.stateful = stateful\n",
    "\n",
    "    def forward(self, xs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, D = xs.shape\n",
    "        D, H = Wx.shape\n",
    "        \n",
    "        self.layers = []\n",
    "        hs = np.empty((N, T, H), dtype='f')\n",
    "        \n",
    "        if not self.stateful or self.h is None:\n",
    "            self.h = np.zeros((N, H), dtype='f')\n",
    "            \n",
    "        for t in range(T):\n",
    "            layer = RNN(*self.params)\n",
    "            self.h = layer.forward(xs[:, t, :], self.h)\n",
    "            hs[:, t, :] = self.h\n",
    "            self.layers.append(layer)\n",
    "        \n",
    "        return hs\n",
    "    \n",
    "    def backward(self, dhs):\n",
    "        Wx, Wh, b = self.params\n",
    "        N, T, H = dhs.shape\n",
    "        D, H = Wx.shape\n",
    "        \n",
    "        dxs = np.empty((N, T, D), dtype='f')\n",
    "        dh = 0\n",
    "        grads = [0, 0, 0]\n",
    "        for t in reversed(range(T)):\n",
    "            layer = self.layers[t]\n",
    "            dx, dh = layer.backward(dhs[:, t, :] + dh)\n",
    "            dxs[:, t, :] = dx\n",
    "            \n",
    "            for i, grad in enumerate(grads):\n",
    "                self.grads[i][...] = grad\n",
    "            self.dh = dh\n",
    "            \n",
    "        return dxs\n",
    "        \n",
    "    def set_state(self, h):\n",
    "        self.h = h\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.h = None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNNLM(RNN Language Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 문장을 단어의 단어들을 단어의 분산 표현(vector)로 변환\n",
    "2. 해당 vector들을 순서대로 RNN layer에 입력\n",
    "3. RNNLM은 지금까지 입력된 단어를 기억하고 다음에 출현할 단어를 예측\n",
    "<br/> <img src='../figs/fig%205-25.png'> <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 각 layer 종류 XX마다 Time XX layer를 구현\n",
    "    - Time Affine\n",
    "    - Time Embedding\n",
    "    - Time Softmax with Loss : T개의 Softmax with Loss layer 각각의 loss를 평균\n",
    "    - Time Sigmoid with Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-23T06:57:34.783310Z",
     "start_time": "2021-10-23T06:57:34.751628Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.layers import *\n",
    "from common.funcs import cross_entropy_error, softmax, sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Time Affine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-23T06:57:35.399778Z",
     "start_time": "2021-10-23T06:57:35.375522Z"
    }
   },
   "outputs": [],
   "source": [
    "class TimeAffine:\n",
    "    def __init__(self, W, b):\n",
    "        self.params = [W, b]\n",
    "        self.grads = [np.zeros_like(W), np.zeros_like(b)]\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, T, D = x.shape\n",
    "        W, b = self.params\n",
    "        \n",
    "        rx = x.reshape(N*T, -1)\n",
    "        out = np.dot(rx, W) + b\n",
    "        self.x = x\n",
    "        return out.reshape(N, T, -1)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        x = self.x\n",
    "        N, T, D = x.shape\n",
    "        W, b = self.params\n",
    "        \n",
    "        dout = dout.reshape(N*T, -1)\n",
    "        rx = x.reshape(N*T, -1)\n",
    "        \n",
    "        dx = np.dot(dout, W.T)\n",
    "        dx = dx.reshape(*x.shape)\n",
    "        dW = np.dot(rx.T, dout)\n",
    "        db = np.sum(dout, axis=0)\n",
    "\n",
    "        self.grads[0][...] = dW\n",
    "        self.grads[1][...] = db\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Time Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-23T06:57:35.864268Z",
     "start_time": "2021-10-23T06:57:35.837827Z"
    }
   },
   "outputs": [],
   "source": [
    "class TimeEmbedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]\n",
    "        self.grads = [np.zeros_like(W)]\n",
    "        self.layers = None\n",
    "        self.W = W\n",
    "        \n",
    "    def forward(self, xs):\n",
    "        N, T = xs.shape\n",
    "        V, D = self.W.shape\n",
    "\n",
    "        output = np.empty((N, T, D), dtype='f')\n",
    "        self.layers = []\n",
    "        \n",
    "        for t in range(T):\n",
    "            layer = Embedding(self.W)\n",
    "            output[:, t, :] = layer.forward(xs[:, t])\n",
    "            self.layers.append(layer)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        N, T, D = dout.shape\n",
    "        \n",
    "        grad = 0\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            layer.backward(dout[:, t, :])\n",
    "            grad += layer.grads[0]\n",
    "            \n",
    "        self.grads[0][...] = grad\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Time Softmax with Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-23T07:04:16.903180Z",
     "start_time": "2021-10-23T07:04:16.874774Z"
    }
   },
   "outputs": [],
   "source": [
    "class TimeSoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.cache = None\n",
    "        self.ignore_label = -1\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        N, T, V = xs.shape\n",
    "        \n",
    "        if ts.ndim == 3: # 정답 label이 one-hot vector일 때\n",
    "            ts = ts.argmax(axis=2)\n",
    "        \n",
    "        # ignore_label에 해당하는 데이터 처리\n",
    "        mask = (ts != self.ignore_label)\n",
    "        \n",
    "        xs = xs.reshape(N * T, V)\n",
    "        ts = ts.reshape(N * T)\n",
    "        mask = mask.reshape(N * T)\n",
    "        \n",
    "        ys = softmax(xs)\n",
    "        ls = mask * np.log(ys[np.arange(N * T), ts]) # ignore_label에 해당하는 데이터 손실 0으로 처리\n",
    "        loss = -np.sum(ls) / mask.sum()\n",
    "        \n",
    "        self.cache = (ts, ys, mask, (N, T, V))\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def backward(self, dout=1):\n",
    "        ts, ys, mask, (N, T, V) = self.cache\n",
    "        \n",
    "        dx = ys\n",
    "        dx[np.arange(N * T), ts] -= 1\n",
    "        dx = dx * dout / mask.sum()\n",
    "        dx *= mask[:, np.newaxis] # ignoer_label에 해당하는 데이터 기울기를 0으로\n",
    "        \n",
    "        dx = dx.reshape((N, T, V))\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Time Sigmoid with Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-23T07:04:17.647899Z",
     "start_time": "2021-10-23T07:04:17.629149Z"
    }
   },
   "outputs": [],
   "source": [
    "class TimeSigmoidWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.xs_shape = None\n",
    "        self.layers= None\n",
    "\n",
    "    def forward(self, xs, ts):\n",
    "        N, T = xs.shape\n",
    "        self.xs_shape = xs.shape\n",
    "        \n",
    "        self.layers = []\n",
    "        loss = 0\n",
    "        \n",
    "        for t in range(T):\n",
    "            layer = SigmoidWithLoss()\n",
    "            loss += layer.forward(xs[:, t], ts[:, t])\n",
    "            self.layers.append(layer)\n",
    "            \n",
    "        return loss/T\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        N, T = self.xs_shape\n",
    "        dxs = np.empty(self.xs_shape, dtype='f')\n",
    "        \n",
    "        dout /= T\n",
    "        \n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            dxs[:, t] = layer.backward(dout)\n",
    "            \n",
    "        return dxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNNLM 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SimpleRnnlm : 아래와 같이 4개의 time layer를 쌓은 신경망\n",
    "<br/> <img src = '../figs/fig%205-30.png' width='300px'> <br/>\n",
    "- Xavier initialization : 이전 layer의 노드의 개수($n$)을 사용해 표준편차가 $1 \\over \\sqrt n$인 분포로 initialize\n",
    "    - non-linear(tanh, sigmoid 등)에서 좋은 성능\n",
    "    - ReLU의 경우 출력값이 0으로 수렴하는 문제가 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-23T07:02:53.150693Z",
     "start_time": "2021-10-23T07:02:53.114733Z"
    }
   },
   "outputs": [],
   "source": [
    "class SimpleRnnlm:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "        \n",
    "        # initialize weight(Xavier)\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        rnn_Wx = (rn(D, H) / np.sqrt(D)).astype('f')\n",
    "        rnn_Wh = (rn(H, H) / np.sqrt(H)).astype('f')\n",
    "        rnn_b = np.zeros(H).astype('f')\n",
    "        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "        \n",
    "        # layer 생성\n",
    "        self.layers = [\n",
    "            TimeEmbedding(embed_W),\n",
    "            TimeRNN(rnn_Wx, rnn_Wh, rnn_b, stateful=True),\n",
    "            TimeAffine(affine_W, affine_b)\n",
    "        ]\n",
    "        self.loss_layer = TimeSoftmaxWithLoss()\n",
    "        self.rnn_layer = self.layers[1]\n",
    "        \n",
    "        # list\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "            \n",
    "        \n",
    "    def forward(self, xs, ts):\n",
    "        for layer in self.layers:\n",
    "            xs = layer.forward(xs)\n",
    "        loss = self.loss_layer.forward(xs, ts)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "            \n",
    "        return dout\n",
    "    \n",
    "    def reset_state(self):\n",
    "        '''\n",
    "        hidden state 초기화\n",
    "        '''\n",
    "        self.rnn_layer.reset_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- '확률의 역수' : 값이 낮을수록 언어 모델의 예측 성능이 좋은 모델\n",
    "- $perplexity = e^L =  exp~(- {1 \\over N} \\sum\\limits_{n} \\sum\\limits_{k} t_{nk}~log y_{nk})$\n",
    "- $N$ : 데이터의 총 개수, $t_n$ : 정답 레이블, $t_{nk}$ : n개째 데이터의 k번째 값, $y_{nk}$ : 확률분포\n",
    "- 위 값들을 통해 계산한 $L$ 은 Cross-entroy error를 통해 계산한 신경망의 손실과 같음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-23T07:02:53.900907Z",
     "start_time": "2021-10-23T07:02:53.893048Z"
    }
   },
   "outputs": [],
   "source": [
    "def perplexity():\n",
    "    d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-23T07:02:54.668258Z",
     "start_time": "2021-10-23T07:02:54.654647Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from common.optimizer import SGD\n",
    "from data import ptb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-23T07:06:59.079418Z",
     "start_time": "2021-10-23T07:06:59.064089Z"
    }
   },
   "outputs": [],
   "source": [
    "# hyperparameter 설정\n",
    "batch_size = 10\n",
    "wordvec_size = 100\n",
    "hidden_size = 100 # RNN의 hidden state의 벡터의 길이\n",
    "time_size = 5 # Truncated BPTT가 한번에 펼치는 시간의 길이\n",
    "lr = 0.1\n",
    "max_epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-23T07:07:00.052766Z",
     "start_time": "2021-10-23T07:07:00.006062Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus size : 1000, vocab size : 418\n"
     ]
    }
   ],
   "source": [
    "# data load\n",
    "corpus, word_to_id, id_to_word = ptb.load_data('train')\n",
    "corpus_size = 1000\n",
    "corpus = corpus[:corpus_size]\n",
    "vocab_size = int(max(corpus)+1)\n",
    "\n",
    "xs = corpus[:-1]\n",
    "ts = corpus[1:]\n",
    "data_size = len(xs)\n",
    "print(f'corpus size : {corpus_size}, vocab size : {vocab_size}')\n",
    "\n",
    "# 변수 설정\n",
    "max_iters = data_size // (batch_size * time_size)\n",
    "time_idx = 0\n",
    "total_loss = 0\n",
    "loss_count = 0\n",
    "ppl_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-23T07:07:01.189713Z",
     "start_time": "2021-10-23T07:07:01.164348Z"
    }
   },
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "model = SimpleRnnlm(vocab_size, wordvec_size, hidden_size)\n",
    "optimizer = SGD(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-23T07:07:14.884207Z",
     "start_time": "2021-10-23T07:07:02.055882Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch : 1 | perplexity : 414.6209755862757 |\n",
      "| epoch : 2 | perplexity : 406.1885653513905 |\n",
      "| epoch : 3 | perplexity : 395.4628658685468 |\n",
      "| epoch : 4 | perplexity : 378.35236129675485 |\n",
      "| epoch : 5 | perplexity : 366.1856170157493 |\n",
      "| epoch : 6 | perplexity : 354.191880975869 |\n",
      "| epoch : 7 | perplexity : 344.57856882471776 |\n",
      "| epoch : 8 | perplexity : 331.4737590365354 |\n",
      "| epoch : 9 | perplexity : 318.2593743052612 |\n",
      "| epoch : 10 | perplexity : 305.9840191114381 |\n",
      "| epoch : 11 | perplexity : 292.81834082706536 |\n",
      "| epoch : 12 | perplexity : 281.54935074136347 |\n",
      "| epoch : 13 | perplexity : 269.83657105525566 |\n",
      "| epoch : 14 | perplexity : 258.98942684398656 |\n",
      "| epoch : 15 | perplexity : 248.69355655552775 |\n",
      "| epoch : 16 | perplexity : 236.7400486161392 |\n",
      "| epoch : 17 | perplexity : 224.27638588249593 |\n",
      "| epoch : 18 | perplexity : 213.92731151485285 |\n",
      "| epoch : 19 | perplexity : 205.62306209405713 |\n",
      "| epoch : 20 | perplexity : 200.49764476537203 |\n",
      "| epoch : 21 | perplexity : 193.66021180447368 |\n",
      "| epoch : 22 | perplexity : 185.24640673793252 |\n",
      "| epoch : 23 | perplexity : 177.87764930207234 |\n",
      "| epoch : 24 | perplexity : 176.71877856818656 |\n",
      "| epoch : 25 | perplexity : 171.8442929574679 |\n",
      "| epoch : 26 | perplexity : 169.5780501093459 |\n",
      "| epoch : 27 | perplexity : 163.2297116439843 |\n",
      "| epoch : 28 | perplexity : 160.26484787255706 |\n",
      "| epoch : 29 | perplexity : 157.79490143703734 |\n",
      "| epoch : 30 | perplexity : 151.4927692984216 |\n",
      "| epoch : 31 | perplexity : 151.15472204168918 |\n",
      "| epoch : 32 | perplexity : 147.07616862526393 |\n",
      "| epoch : 33 | perplexity : 145.73098564342956 |\n",
      "| epoch : 34 | perplexity : 142.2625787695353 |\n",
      "| epoch : 35 | perplexity : 141.33875453848015 |\n",
      "| epoch : 36 | perplexity : 136.56606246907015 |\n",
      "| epoch : 37 | perplexity : 133.01644904368538 |\n",
      "| epoch : 38 | perplexity : 131.25907531062202 |\n",
      "| epoch : 39 | perplexity : 127.33125837370964 |\n",
      "| epoch : 40 | perplexity : 124.1875102115692 |\n",
      "| epoch : 41 | perplexity : 125.31008391831422 |\n",
      "| epoch : 42 | perplexity : 120.38473039111155 |\n",
      "| epoch : 43 | perplexity : 116.42880476053062 |\n",
      "| epoch : 44 | perplexity : 114.70994728076741 |\n",
      "| epoch : 45 | perplexity : 112.33215879557697 |\n",
      "| epoch : 46 | perplexity : 112.39150414563538 |\n",
      "| epoch : 47 | perplexity : 108.3106393785315 |\n",
      "| epoch : 48 | perplexity : 103.90198683930005 |\n",
      "| epoch : 49 | perplexity : 102.49206830972331 |\n",
      "| epoch : 50 | perplexity : 100.55252590782936 |\n",
      "| epoch : 51 | perplexity : 97.85794417537893 |\n",
      "| epoch : 52 | perplexity : 96.61123257945378 |\n",
      "| epoch : 53 | perplexity : 94.62375701445521 |\n",
      "| epoch : 54 | perplexity : 92.63119854691425 |\n",
      "| epoch : 55 | perplexity : 91.51686542501174 |\n",
      "| epoch : 56 | perplexity : 87.66390386869016 |\n",
      "| epoch : 57 | perplexity : 86.67441282516366 |\n",
      "| epoch : 58 | perplexity : 83.36943705594857 |\n",
      "| epoch : 59 | perplexity : 81.72809798066285 |\n",
      "| epoch : 60 | perplexity : 79.90400673665046 |\n",
      "| epoch : 61 | perplexity : 79.26315091445119 |\n",
      "| epoch : 62 | perplexity : 77.05667269685765 |\n",
      "| epoch : 63 | perplexity : 73.16542483516096 |\n",
      "| epoch : 64 | perplexity : 72.47572513522775 |\n",
      "| epoch : 65 | perplexity : 72.29264414082606 |\n",
      "| epoch : 66 | perplexity : 69.84781760776154 |\n",
      "| epoch : 67 | perplexity : 68.97754823041812 |\n",
      "| epoch : 68 | perplexity : 64.8410835687248 |\n",
      "| epoch : 69 | perplexity : 63.691214355439584 |\n",
      "| epoch : 70 | perplexity : 62.69043040724442 |\n",
      "| epoch : 71 | perplexity : 61.38031229214043 |\n",
      "| epoch : 72 | perplexity : 59.56395567863522 |\n",
      "| epoch : 73 | perplexity : 58.541966739737866 |\n",
      "| epoch : 74 | perplexity : 57.16137317953002 |\n",
      "| epoch : 75 | perplexity : 56.89150491214917 |\n",
      "| epoch : 76 | perplexity : 53.91030769601761 |\n",
      "| epoch : 77 | perplexity : 53.21597583953492 |\n",
      "| epoch : 78 | perplexity : 51.56915448669411 |\n",
      "| epoch : 79 | perplexity : 50.18792721297824 |\n",
      "| epoch : 80 | perplexity : 48.50528362369456 |\n",
      "| epoch : 81 | perplexity : 48.06277875069828 |\n",
      "| epoch : 82 | perplexity : 47.92769972223785 |\n",
      "| epoch : 83 | perplexity : 45.06740292097846 |\n",
      "| epoch : 84 | perplexity : 44.62081376909832 |\n",
      "| epoch : 85 | perplexity : 43.85651019032933 |\n",
      "| epoch : 86 | perplexity : 42.480526185347024 |\n",
      "| epoch : 87 | perplexity : 42.048636850470544 |\n",
      "| epoch : 88 | perplexity : 39.95544879188507 |\n",
      "| epoch : 89 | perplexity : 38.541989052749116 |\n",
      "| epoch : 90 | perplexity : 37.630963039952974 |\n",
      "| epoch : 91 | perplexity : 37.556579456676694 |\n",
      "| epoch : 92 | perplexity : 36.3231099137912 |\n",
      "| epoch : 93 | perplexity : 35.859508018861995 |\n",
      "| epoch : 94 | perplexity : 35.24834062981909 |\n",
      "| epoch : 95 | perplexity : 34.007155712771805 |\n",
      "| epoch : 96 | perplexity : 33.07419612580433 |\n",
      "| epoch : 97 | perplexity : 32.46896509457438 |\n",
      "| epoch : 98 | perplexity : 31.148285495913306 |\n",
      "| epoch : 99 | perplexity : 30.593678689564538 |\n",
      "| epoch : 100 | perplexity : 29.621558922494202 |\n"
     ]
    }
   ],
   "source": [
    "# 각 mini-batch에서 샘플을 읽는 시작 위치 계산\n",
    "jump = (corpus_size - 1) // batch_size\n",
    "offsets = [i * jump for i in range(batch_size)]\n",
    "\n",
    "# 각 epoch, iter마다 순차적으로\n",
    "for epoch in range(max_epoch):\n",
    "    for iters in range(max_iters):\n",
    "        # mini-batch 생성\n",
    "        batch_x = np.empty((batch_size, time_size), dtype='i')\n",
    "        batch_t = np.empty((batch_size, time_size), dtype='i')\n",
    "        for t in range(time_size):\n",
    "            for i, offset in enumerate(offsets):\n",
    "                batch_x[i, t] = xs[(offset + time_idx) % data_size]\n",
    "                batch_t[i, t] = ts[(offset + time_idx) % data_size]\n",
    "            time_idx += 1\n",
    "            \n",
    "        # 기울기 갱신\n",
    "        loss = model.forward(batch_x, batch_t)\n",
    "        model.backward()\n",
    "        optimizer.update(model.params, model.grads)\n",
    "        total_loss += loss\n",
    "        loss_count += 1\n",
    "    \n",
    "    # 각 epoch마다 평가(perplexity)\n",
    "    ppl = np.exp(total_loss / loss_count)\n",
    "    print(f'| epoch : {epoch + 1} | perplexity : {ppl} |')\n",
    "    ppl_list.append(float(ppl))\n",
    "    total_loss, loss_count = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-23T07:07:18.203922Z",
     "start_time": "2021-10-23T07:07:17.766363Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x23e4bb6b308>]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(ppl_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
