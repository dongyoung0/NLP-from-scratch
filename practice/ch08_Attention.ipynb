{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seq2seq의 문제점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- seq2seq은 Encoder가 문장을 '고정 길이'의 벡터로 변환해서 Decoder에 전달함.  \n",
    "문장 길이와 관계 없이 '고정 길이'로 변환하기 때문에 긴 문장의 경우 필요한 정보가 다 담기지 못하는 문제가 발생함.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Endoder 개선\n",
    "- 개선 방법으로, 각 단어 (시각)별 hidden state $h$를 모두 출력($hs$)\n",
    "<br/> <img src='../figs/fig%208-2.png'> <br/>\n",
    "- 이 떄, 각 시각의 $h$에는 직전에 입력된 단어(Encoder의 input)에 대한 정보가 많이 포함되어 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder 개선\n",
    "- Encoder가 각 단어에 대응하는 LSTM layer의 hidden state $h$를 모아서 $hs$로 출력해서 Decoder에 전달하면, Decoder는 $hs$를 다시 문장으로 변환함.\n",
    "<br/> <img src='../figs/fig%208-4.png'> <br/>\n",
    "- 이 과정에서 '입력과 출력의 여러 단어 중 어떤 단어끼리 서로 관련되어 있는가?'(alignment)를 학습시키고자 함.  \n",
    "'도착어 단어'(I)와 대응 관계에 있는 '출발어 단어'(나)의 정보를 골라내기 위해 Attention 매커니즘을 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted Sum\n",
    "- Decoder의 각 시각마다, 입력된 단어와 대응 관계인 단어의 벡터를 $hs$에서 골라내고자 함.  \n",
    "그러나 $hs$에서 벡터를 선택하는 작업은 미분할 수 없기 때문에 back-propagation을 위해 미분 가능한 연산으로 대체\n",
    "- 각 단어의 중요도를 나타내는 가중치 $a$와 각 단어의 벡터 $hs$로부터 weighted sum을 구하여 맥락 벡터 $c$를 얻음\n",
    "<br/> <img src='../figs/fig%208-8.png'> <br/>\n",
    "- $c$에 현 시각의 변환을 수행하는데 필요한 정보(단어간의 대응 관계, alignment)가 담기는 방향으로 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T06:21:29.545617Z",
     "start_time": "2021-11-03T06:21:29.533676Z"
    }
   },
   "outputs": [],
   "source": [
    "class WeightSum:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.cache = None\n",
    "    \n",
    "    def forward(self, hs, a):\n",
    "        N, T, H = hs.shape\n",
    "        \n",
    "        ar = a.reshape(N, T, 1).repeat(H, axis=2)\n",
    "        t = hs * ar\n",
    "        c = np.sum(t, axis=1)\n",
    "        \n",
    "        self.cache = (hs, ar)\n",
    "        \n",
    "        return c\n",
    "    \n",
    "    def backward(self, dc):\n",
    "        hs, ar = self.cache\n",
    "        N, T, H = hs.shape\n",
    "        \n",
    "        dt = dc.reshape(N, 1, H).repeat(T, axis=1)\n",
    "        dar = dt * hs\n",
    "        dhs = dt * ar\n",
    "        da = np.sum(dar, axis=2)\n",
    "        \n",
    "        return dhs, da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a를 구하는 방법\n",
    "- Decoder의 LSTM layer의 $h$가 Encoder의 출력 $hs$의 각 단어 벡터와 얼마나 '비슷한지'를 수치로 나타내서 가중치로 이용.  \n",
    "- $h$와 $hs$의 내적을 통해 유사도 점수 $s$를 구하고, Softmax를 통해 정규화해서 가중치 $a$를 구함.\n",
    "<br/> <img src='../figs/fig%208-15.png'> <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T06:21:30.596797Z",
     "start_time": "2021-11-03T06:21:30.577848Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.np import *\n",
    "from common.layers import Softmax\n",
    "\n",
    "class AttentionWeight:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.softmax = Softmax()\n",
    "        self.cache = None\n",
    "        \n",
    "    def forward(self, hs, h):\n",
    "        N, T, H = hs.shape\n",
    "        \n",
    "        hr = h.reshape(N, 1, H).repeat(T, axis=1)\n",
    "        t = hs * hr\n",
    "        s = np.sum(t, axis=2)\n",
    "        a = self.softmax.forward(s)\n",
    "        \n",
    "        self.cache = (hs, hr)\n",
    "        \n",
    "        return a\n",
    "    \n",
    "    def backward(self, da):\n",
    "        hs, hr = self.cache\n",
    "        N, T, H = hs.shape\n",
    "        \n",
    "        ds = self.softmax.backward(da)\n",
    "        dt = ds.reshape(N, T, 1).repeat(H, axis=2)\n",
    "        dhs = dt*hr\n",
    "        dhr = dt*hs\n",
    "        dh = np.sum(dhr, axis=1)\n",
    "        \n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 결합\n",
    "- 위에서 구현한 Weight Sum, Attention Weight를 하나로 결합.\n",
    "<br/> <img src='../figs/fig%208-16.png'> <br/>\n",
    "- Attention Weight layer가 Encoder의 출력 $hs$에 주목하여 각 단어의 가중치 $a$를 구하고, $a$와 $hs$의 Weighted Sum을 통해 맥락 벡터 $c$를 구함\n",
    "- 이 과정을 수행하는 layer를 Attention layer라고 부름.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T06:28:56.838920Z",
     "start_time": "2021-11-03T06:28:56.830916Z"
    }
   },
   "outputs": [],
   "source": [
    " class Attention:\n",
    "        def __init__(self):\n",
    "            self.params, self.grads = [], []\n",
    "            self.attention_weight_layer = AttentionWeight()\n",
    "            self.weight_sum_layer = WeightSum()\n",
    "            self.attention_weight = None\n",
    "            \n",
    "        def forward(self, hs, h):\n",
    "            a = self.attention_weight_layer.forward(hs, h)\n",
    "            out = self.weight_sum_layer.forward(hs, a)\n",
    "            self.attention_weight = a\n",
    "            \n",
    "            return out\n",
    "        \n",
    "        def backward(self, dout):\n",
    "            dhs0, da = self.weight_sum_layer.backward(dout)\n",
    "            dhs1, dh = self.attention_weight_layer.backward(da)\n",
    "            dhs = dhs0 + dhs1\n",
    "            \n",
    "            return dhs, dh      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 앞선 layer들과 마찬가지로 Time Attention layer 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T07:54:15.026803Z",
     "start_time": "2021-11-03T07:54:15.018820Z"
    }
   },
   "outputs": [],
   "source": [
    "class TimeAttention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.layers = None\n",
    "        self.attention_weights = None\n",
    "        \n",
    "    def forward(self, hs_enc, hs_dec):\n",
    "        N, T, H = hs_dec.shape\n",
    "        out = np.empty_like(hs_dec)\n",
    "        self.layers = []\n",
    "        self.attention_weights = []\n",
    "        \n",
    "        for t in range(T):\n",
    "            layer = Attention()\n",
    "            out[:, t, :] = layer.forward(hs_enc, hs_dec[:, t, :])\n",
    "            self.layers.append(layer)\n",
    "            self.attention_weights.append(layer.attention_weight)\n",
    "            \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        N, T, H = dout.shape\n",
    "        dhs_enc = 0\n",
    "        dhs_dec = np.empty_like(dout)\n",
    "        \n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            dhs, dh = layer.backward(dout[:, t, :])\n",
    "            dhs_enc += dhs\n",
    "            dhs_dec[:, t, :] = dh\n",
    "            \n",
    "        return dhs_enc, dhs_dec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  기존 Affine layer와 LSTM layer 사이에 Attention layer를 넣어서 Decoder에 어텐션 정보를 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention을 포함한 seq2seq 구현\n",
    "- 앞에서 구현해놓은 Enccoder에서 LSTM layer의 마지막 $h$만 반환하던 것에서 모든 $h$를 반환하는 것으로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T07:54:17.244087Z",
     "start_time": "2021-11-03T07:54:17.232118Z"
    }
   },
   "outputs": [],
   "source": [
    "sys.path.append('../code/seq2seq')\n",
    "from seq2seq import Encoder, Seq2seq\n",
    "from common.time_layers import *\n",
    "\n",
    "class AttentionEncoder(Encoder):\n",
    "    def forward(self, xs):\n",
    "        xs = self.embed.forward(xs)\n",
    "        hs = self.lstm.forward(xs)\n",
    "        return hs\n",
    "    \n",
    "    def backward(self, dhs):\n",
    "        dout = self.lstm.backward(dhs)\n",
    "        dout = self.embed.backward(dout)\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Decoder의 경우 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T07:54:17.646896Z",
     "start_time": "2021-11-03T07:54:17.619637Z"
    }
   },
   "outputs": [],
   "source": [
    "class AttentionDecoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "        embed_W = (rn(V, D)/100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4*H)/np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4*H)/np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4*H).astype('f')\n",
    "        affine_W = (rn(2*H, V)/np.sqrt(2*H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "        \n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
    "        # attention layer 추가\n",
    "        self.attention = TimeAttention()\n",
    "        self.affine = TimeAffine(affine_W, affine_b)\n",
    "        layers = [self.embed, self.lstm, self.attention, self.affine]\n",
    "        \n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "        \n",
    "    \n",
    "    def forward(self, xs, enc_hs):\n",
    "        h = enc_hs[:,-1]\n",
    "        self.lstm.set_state(h)\n",
    "        \n",
    "        out = self.embed.forward(xs)\n",
    "        dec_hs = self.lstm.forward(out)\n",
    "        # attention layer를 통해 c 계산\n",
    "        c = self.attention.forward(enc_hs, dec_hs)\n",
    "        out = np.concatenate((c, dec_hs), axis=2)\n",
    "        score = self.affine.forward(out)\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def backward(self, dscore):\n",
    "        dout = self.affine.backward(dscore)\n",
    "        N, T, H2 = dout.shape\n",
    "        H = H2 // 2\n",
    "\n",
    "        dc, ddec_hs0 = dout[:,:,:H], dout[:,:,H:]\n",
    "        denc_hs, ddec_hs1 = self.attention.backward(dc)\n",
    "        ddec_hs = ddec_hs0 + ddec_hs1\n",
    "        dout = self.lstm.backward(ddec_hs)\n",
    "        dh = self.lstm.dh\n",
    "        denc_hs[:, -1] += dh\n",
    "        self.embed.backward(dout)\n",
    "\n",
    "        return denc_hs\n",
    "\n",
    "    def generate(self, enc_hs, start_id, sample_size):\n",
    "        sampled = []\n",
    "        sample_id = start_id\n",
    "        h = enc_hs[:, -1]\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        for _ in range(sample_size):\n",
    "            x = np.array([sample_id]).reshape((1, 1))\n",
    "\n",
    "            out = self.embed.forward(x)\n",
    "            dec_hs = self.lstm.forward(out)\n",
    "            c = self.attention.forward(enc_hs, dec_hs)\n",
    "            out = np.concatenate((c, dec_hs), axis=2)\n",
    "            score = self.affine.forward(out)\n",
    "\n",
    "            sample_id = np.argmax(score.flatten())\n",
    "            sampled.append(sample_id)\n",
    "\n",
    "        return sampled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위 Attention Encoder와 Attention Decoder를 결합하여 Attention Seq2seq 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T07:54:18.181532Z",
     "start_time": "2021-11-03T07:54:18.169568Z"
    }
   },
   "outputs": [],
   "source": [
    "class AttentionSeq2seq(Seq2seq):\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        args = vocab_size, wordvec_size, hidden_size\n",
    "        self.encoder = AttentionEncoder(*args)\n",
    "        self.decoder = AttentionDecoder(*args)\n",
    "        self.softmax = TimeSoftmaxWithLoss()\n",
    "        \n",
    "        self.params = self.encoder.params + self.decoder.params\n",
    "        self.grads = self.encoder.grads + self.decoder.grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 날짜 형식 변환 문제\n",
    "- 날짜 형식 변환 문제를 통해 위에서 구현한 Attention seq2seq의 성능을 평가해 볼 것.\n",
    "<br/> <img src='../figs/fig%208-22.png'> <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T07:54:18.798516Z",
     "start_time": "2021-11-03T07:54:18.794495Z"
    }
   },
   "outputs": [],
   "source": [
    "from data import sequence\n",
    "from common.optimizer import Adam\n",
    "from common.trainer import Trainer\n",
    "from common.util import eval_seq2seq\n",
    "from seq2seq import PeekySeq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T07:54:19.837478Z",
     "start_time": "2021-11-03T07:54:19.219837Z"
    }
   },
   "outputs": [],
   "source": [
    "# data load\n",
    "(x_train, t_train), (x_test, t_test) = sequence.load_data('date.txt')\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "\n",
    "# reverse\n",
    "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
    "\n",
    "# hyperparameter 설정\n",
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 256\n",
    "batch_size = 128\n",
    "max_epoch = 10\n",
    "max_grad = 5.0\n",
    "\n",
    "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T09:14:35.304928Z",
     "start_time": "2021-11-03T08:00:03.553439Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 1 |  itr 1/351 | time 0.8640406131744385[s] | loss 4.07801399230957\n",
      "| epoch 1 |  itr 21/351 | time 11.422736883163452[s] | loss 3.0866257762908935\n",
      "| epoch 1 |  itr 41/351 | time 22.832525730133057[s] | loss 1.9038669395446781\n",
      "| epoch 1 |  itr 61/351 | time 36.30640697479248[s] | loss 1.7195860290527343\n",
      "| epoch 1 |  itr 81/351 | time 49.66436815261841[s] | loss 1.4629032897949221\n",
      "| epoch 1 |  itr 101/351 | time 63.245250940322876[s] | loss 1.1921350669860842\n",
      "| epoch 1 |  itr 121/351 | time 76.58332061767578[s] | loss 1.1404548883438113\n",
      "| epoch 1 |  itr 141/351 | time 89.89800238609314[s] | loss 1.0911264801025393\n",
      "| epoch 1 |  itr 161/351 | time 103.42958426475525[s] | loss 1.0587649583816527\n",
      "| epoch 1 |  itr 181/351 | time 116.96468448638916[s] | loss 1.0406064891815185\n",
      "| epoch 1 |  itr 201/351 | time 131.3544158935547[s] | loss 1.0313516998291017\n",
      "| epoch 1 |  itr 221/351 | time 146.01213383674622[s] | loss 1.024596667289734\n",
      "| epoch 1 |  itr 241/351 | time 159.63236451148987[s] | loss 1.0165974426269533\n",
      "| epoch 1 |  itr 261/351 | time 174.34999251365662[s] | loss 1.008022174835205\n",
      "| epoch 1 |  itr 281/351 | time 188.44568848609924[s] | loss 1.002981653213501\n",
      "| epoch 1 |  itr 301/351 | time 203.03828978538513[s] | loss 1.0013097476959227\n",
      "| epoch 1 |  itr 321/351 | time 216.64398503303528[s] | loss 0.9976738929748536\n",
      "| epoch 1 |  itr 341/351 | time 230.68432664871216[s] | loss 0.9975323057174684\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "X 1978-08-11\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 1978-08-11\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "X 1978-08-11\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "X 1978-08-11\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "X 1978-08-11\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "X 1978-08-11\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "X 1978-08-11\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 1978-08-11\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 1978-08-11\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 1978-08-11\n",
      "---\n",
      "val acc : 0.0\n",
      "| epoch 2 |  itr 1/351 | time 0.7569756507873535[s] | loss 1.0001761436462402\n",
      "| epoch 2 |  itr 21/351 | time 15.549349069595337[s] | loss 0.9950354194641111\n",
      "| epoch 2 |  itr 41/351 | time 28.80803918838501[s] | loss 0.990463433265686\n",
      "| epoch 2 |  itr 61/351 | time 41.9483859539032[s] | loss 0.9906761264801025\n",
      "| epoch 2 |  itr 81/351 | time 55.27077889442444[s] | loss 0.9897627878189086\n",
      "| epoch 2 |  itr 101/351 | time 68.82778835296631[s] | loss 0.989800248146057\n",
      "| epoch 2 |  itr 121/351 | time 82.50562357902527[s] | loss 0.9852891921997069\n",
      "| epoch 2 |  itr 141/351 | time 95.90273571014404[s] | loss 0.9817801141738892\n",
      "| epoch 2 |  itr 161/351 | time 109.33499884605408[s] | loss 0.9763122749328612\n",
      "| epoch 2 |  itr 181/351 | time 122.866051197052[s] | loss 0.9650785350799562\n",
      "| epoch 2 |  itr 201/351 | time 136.22636318206787[s] | loss 0.9539524888992309\n",
      "| epoch 2 |  itr 221/351 | time 149.4260733127594[s] | loss 0.9359839534759521\n",
      "| epoch 2 |  itr 241/351 | time 162.896222114563[s] | loss 0.8977420043945313\n",
      "| epoch 2 |  itr 261/351 | time 176.61518168449402[s] | loss 0.8281972241401674\n",
      "| epoch 2 |  itr 281/351 | time 190.1566891670227[s] | loss 0.7382711887359619\n",
      "| epoch 2 |  itr 301/351 | time 203.56097269058228[s] | loss 0.6644377183914185\n",
      "| epoch 2 |  itr 321/351 | time 216.7541708946228[s] | loss 0.5791467380523683\n",
      "| epoch 2 |  itr 341/351 | time 229.77663779258728[s] | loss 0.4649479007720948\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 2006-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 2007-08-09\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 1983-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 2016-11-08\n",
      "---\n",
      "val acc : 51.459999999999994\n",
      "| epoch 3 |  itr 1/351 | time 0.6751952171325684[s] | loss 0.3480678558349609\n",
      "| epoch 3 |  itr 21/351 | time 13.636111974716187[s] | loss 0.29652423024177554\n",
      "| epoch 3 |  itr 41/351 | time 26.587833881378174[s] | loss 0.20665359079837797\n",
      "| epoch 3 |  itr 61/351 | time 39.740877628326416[s] | loss 0.13966187059879304\n",
      "| epoch 3 |  itr 81/351 | time 52.63350558280945[s] | loss 0.09384391099214555\n",
      "| epoch 3 |  itr 101/351 | time 65.69976687431335[s] | loss 0.06695069134235385\n",
      "| epoch 3 |  itr 121/351 | time 78.93159556388855[s] | loss 0.05282401517033576\n",
      "| epoch 3 |  itr 141/351 | time 92.0235481262207[s] | loss 0.03924859672784807\n",
      "| epoch 3 |  itr 161/351 | time 105.03308987617493[s] | loss 0.031494880318641665\n",
      "| epoch 3 |  itr 181/351 | time 118.12117290496826[s] | loss 0.026103296801447868\n",
      "| epoch 3 |  itr 201/351 | time 131.07118844985962[s] | loss 0.02122177183628082\n",
      "| epoch 3 |  itr 221/351 | time 144.0293390750885[s] | loss 0.017669536545872687\n",
      "| epoch 3 |  itr 241/351 | time 158.2966549396515[s] | loss 0.0151598634570837\n",
      "| epoch 3 |  itr 261/351 | time 171.17574501037598[s] | loss 0.013403959758579725\n",
      "| epoch 3 |  itr 281/351 | time 184.34726572036743[s] | loss 0.011104787029325961\n",
      "| epoch 3 |  itr 301/351 | time 197.34757709503174[s] | loss 0.010058447904884818\n",
      "| epoch 3 |  itr 321/351 | time 210.08821368217468[s] | loss 0.008784320540726184\n",
      "| epoch 3 |  itr 341/351 | time 223.01243948936462[s] | loss 0.008083761222660543\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc : 99.9\n",
      "| epoch 4 |  itr 1/351 | time 0.6612319946289062[s] | loss 0.0072284087538719176\n",
      "| epoch 4 |  itr 21/351 | time 13.856172323226929[s] | loss 0.0070117505826056\n",
      "| epoch 4 |  itr 41/351 | time 26.83633327484131[s] | loss 0.006514271460473538\n",
      "| epoch 4 |  itr 61/351 | time 39.503509521484375[s] | loss 0.006023302264511585\n",
      "| epoch 4 |  itr 81/351 | time 52.31742000579834[s] | loss 0.005334652457386255\n",
      "| epoch 4 |  itr 101/351 | time 65.74867296218872[s] | loss 0.005068181175738573\n",
      "| epoch 4 |  itr 121/351 | time 79.23336410522461[s] | loss 0.004643816649913788\n",
      "| epoch 4 |  itr 141/351 | time 92.01186752319336[s] | loss 0.005322172939777374\n",
      "| epoch 4 |  itr 161/351 | time 105.13074588775635[s] | loss 0.004441166147589683\n",
      "| epoch 4 |  itr 181/351 | time 118.25253796577454[s] | loss 0.0039853455871343615\n",
      "| epoch 4 |  itr 201/351 | time 131.3925278186798[s] | loss 0.0037277679704129695\n",
      "| epoch 4 |  itr 221/351 | time 144.42656874656677[s] | loss 0.003469957830384373\n",
      "| epoch 4 |  itr 241/351 | time 157.50421977043152[s] | loss 0.0031995527725666765\n",
      "| epoch 4 |  itr 261/351 | time 170.55798721313477[s] | loss 0.0032595612760633235\n",
      "| epoch 4 |  itr 281/351 | time 183.69807052612305[s] | loss 0.005018817540258169\n",
      "| epoch 4 |  itr 301/351 | time 196.58929705619812[s] | loss 0.0027798830345273015\n",
      "| epoch 4 |  itr 321/351 | time 209.4101665019989[s] | loss 0.0025572346895933154\n",
      "| epoch 4 |  itr 341/351 | time 222.38814449310303[s] | loss 0.00248087290674448\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc : 99.9\n",
      "| epoch 5 |  itr 1/351 | time 0.7998595237731934[s] | loss 0.002182954549789429\n",
      "| epoch 5 |  itr 21/351 | time 14.485414505004883[s] | loss 0.003950979998335242\n",
      "| epoch 5 |  itr 41/351 | time 28.03149437904358[s] | loss 0.0022091213706880806\n",
      "| epoch 5 |  itr 61/351 | time 40.81390619277954[s] | loss 0.002172183776274324\n",
      "| epoch 5 |  itr 81/351 | time 53.58927607536316[s] | loss 0.003606102243065834\n",
      "| epoch 5 |  itr 101/351 | time 66.24373602867126[s] | loss 0.0022468312736600637\n",
      "| epoch 5 |  itr 121/351 | time 79.92574143409729[s] | loss 0.001824158634990454\n",
      "| epoch 5 |  itr 141/351 | time 93.21198749542236[s] | loss 0.003313454454764724\n",
      "| epoch 5 |  itr 161/351 | time 106.89377331733704[s] | loss 0.0018006361741572617\n",
      "| epoch 5 |  itr 181/351 | time 119.78692197799683[s] | loss 0.0017814536439254875\n",
      "| epoch 5 |  itr 201/351 | time 133.1482982635498[s] | loss 0.0016665066312998532\n",
      "| epoch 5 |  itr 221/351 | time 146.58036875724792[s] | loss 0.0016289468249306085\n",
      "| epoch 5 |  itr 241/351 | time 160.16025042533875[s] | loss 0.0016191269550472499\n",
      "| epoch 5 |  itr 261/351 | time 173.1036615371704[s] | loss 0.001552374898456037\n",
      "| epoch 5 |  itr 281/351 | time 186.08087825775146[s] | loss 0.0015303724724799396\n",
      "| epoch 5 |  itr 301/351 | time 199.20096516609192[s] | loss 0.0014101655548438429\n",
      "| epoch 5 |  itr 321/351 | time 212.62134718894958[s] | loss 0.0013468941487371924\n",
      "| epoch 5 |  itr 341/351 | time 225.74344682693481[s] | loss 0.0013949284236878154\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc : 99.92\n",
      "| epoch 6 |  itr 1/351 | time 0.6403138637542725[s] | loss 0.0011220933869481088\n",
      "| epoch 6 |  itr 21/351 | time 13.5803382396698[s] | loss 0.0012896362273022532\n",
      "| epoch 6 |  itr 41/351 | time 26.667996883392334[s] | loss 0.0012062263209372758\n",
      "| epoch 6 |  itr 61/351 | time 39.97763657569885[s] | loss 0.0013441882608458398\n",
      "| epoch 6 |  itr 81/351 | time 52.943927526474[s] | loss 0.0011214815312996507\n",
      "| epoch 6 |  itr 101/351 | time 66.22257208824158[s] | loss 0.0010768972663208842\n",
      "| epoch 6 |  itr 121/351 | time 79.3764295578003[s] | loss 0.0024100934946909543\n",
      "| epoch 6 |  itr 141/351 | time 92.77781343460083[s] | loss 0.0011609843792393802\n",
      "| epoch 6 |  itr 161/351 | time 106.2113389968872[s] | loss 0.0010670152958482503\n",
      "| epoch 6 |  itr 181/351 | time 119.07216620445251[s] | loss 0.001033151876181364\n",
      "| epoch 6 |  itr 201/351 | time 132.54416942596436[s] | loss 0.0009243438160046935\n",
      "| epoch 6 |  itr 221/351 | time 146.01176619529724[s] | loss 0.0008824899699538946\n",
      "| epoch 6 |  itr 241/351 | time 159.02694654464722[s] | loss 0.0009912936086766424\n",
      "| epoch 6 |  itr 261/351 | time 172.36479592323303[s] | loss 0.0009433348383754491\n",
      "| epoch 6 |  itr 281/351 | time 185.6286039352417[s] | loss 0.0009042736166156829\n",
      "| epoch 6 |  itr 301/351 | time 198.99505853652954[s] | loss 0.0008714670292101802\n",
      "| epoch 6 |  itr 321/351 | time 211.97363805770874[s] | loss 0.0033278927882201964\n",
      "| epoch 6 |  itr 341/351 | time 225.22203016281128[s] | loss 0.000824052223470062\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc : 99.92\n",
      "| epoch 7 |  itr 1/351 | time 0.6831746101379395[s] | loss 0.0007817970588803291\n",
      "| epoch 7 |  itr 21/351 | time 13.840169429779053[s] | loss 0.000740204555913806\n",
      "| epoch 7 |  itr 41/351 | time 27.066272258758545[s] | loss 0.0007716721040196717\n",
      "| epoch 7 |  itr 61/351 | time 41.9461784362793[s] | loss 0.0007734791282564402\n",
      "| epoch 7 |  itr 81/351 | time 55.79680275917053[s] | loss 0.0007849326869472862\n",
      "| epoch 7 |  itr 101/351 | time 68.88645195960999[s] | loss 0.0019162407005205748\n",
      "| epoch 7 |  itr 121/351 | time 81.19812226295471[s] | loss 0.0007059732149355114\n",
      "| epoch 7 |  itr 141/351 | time 93.71756196022034[s] | loss 0.0006593056395649909\n",
      "| epoch 7 |  itr 161/351 | time 108.13952422142029[s] | loss 0.0007374396524392069\n",
      "| epoch 7 |  itr 181/351 | time 121.62220907211304[s] | loss 0.0006792371487244964\n",
      "| epoch 7 |  itr 201/351 | time 135.03331661224365[s] | loss 0.0006903766351751982\n",
      "| epoch 7 |  itr 221/351 | time 148.28166818618774[s] | loss 0.0017610296420753\n",
      "| epoch 7 |  itr 241/351 | time 161.64832472801208[s] | loss 0.0005831450689584017\n",
      "| epoch 7 |  itr 261/351 | time 175.37578678131104[s] | loss 0.0006434279331006109\n",
      "| epoch 7 |  itr 281/351 | time 188.88036060333252[s] | loss 0.0005304368655197322\n",
      "| epoch 7 |  itr 301/351 | time 202.32910990715027[s] | loss 0.0016135852225124835\n",
      "| epoch 7 |  itr 321/351 | time 215.60851454734802[s] | loss 0.0005732124648056923\n",
      "| epoch 7 |  itr 341/351 | time 228.89013671875[s] | loss 0.0005945797380991281\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc : 99.92\n",
      "| epoch 8 |  itr 1/351 | time 0.7886159420013428[s] | loss 0.0004389991983771324\n",
      "| epoch 8 |  itr 21/351 | time 14.595706224441528[s] | loss 0.0006174624199047683\n",
      "| epoch 8 |  itr 41/351 | time 28.04745101928711[s] | loss 0.0004963697749190033\n",
      "| epoch 8 |  itr 61/351 | time 41.85792326927185[s] | loss 0.0004858802235685289\n",
      "| epoch 8 |  itr 81/351 | time 55.51135516166687[s] | loss 0.00048280203016474847\n",
      "| epoch 8 |  itr 101/351 | time 69.44629693031311[s] | loss 0.0015454904548823833\n",
      "| epoch 8 |  itr 121/351 | time 82.60579490661621[s] | loss 0.0014399455732200294\n",
      "| epoch 8 |  itr 141/351 | time 95.99049592018127[s] | loss 0.0004447133431676774\n",
      "| epoch 8 |  itr 161/351 | time 109.09495735168457[s] | loss 0.00042632165364921094\n",
      "| epoch 8 |  itr 181/351 | time 122.246173620224[s] | loss 0.0005239437893033029\n",
      "| epoch 8 |  itr 201/351 | time 136.1739580631256[s] | loss 0.0005279541434720158\n",
      "| epoch 8 |  itr 221/351 | time 149.42078971862793[s] | loss 0.00041320001357235\n",
      "| epoch 8 |  itr 241/351 | time 162.54036831855774[s] | loss 0.0004090764676220715\n",
      "| epoch 8 |  itr 261/351 | time 175.81470441818237[s] | loss 0.0013410293404012917\n",
      "| epoch 8 |  itr 281/351 | time 188.80936312675476[s] | loss 0.0004067285382188857\n",
      "| epoch 8 |  itr 301/351 | time 203.28772282600403[s] | loss 0.00042977633071131997\n",
      "| epoch 8 |  itr 321/351 | time 217.4016273021698[s] | loss 0.0004217300389427692\n",
      "| epoch 8 |  itr 341/351 | time 230.92648029327393[s] | loss 0.00039471523370593787\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc : 99.96000000000001\n",
      "| epoch 9 |  itr 1/351 | time 0.6941773891448975[s] | loss 0.0005494490731507539\n",
      "| epoch 9 |  itr 21/351 | time 13.683534860610962[s] | loss 0.0004045340430457145\n",
      "| epoch 9 |  itr 41/351 | time 27.168009519577026[s] | loss 0.0012456762092188\n",
      "| epoch 9 |  itr 61/351 | time 41.42052435874939[s] | loss 0.00036094885435886684\n",
      "| epoch 9 |  itr 81/351 | time 54.842023849487305[s] | loss 0.00037857426330447204\n",
      "| epoch 9 |  itr 101/351 | time 68.6961567401886[s] | loss 0.0003691952303051949\n",
      "| epoch 9 |  itr 121/351 | time 82.85506916046143[s] | loss 0.00032679145690053704\n",
      "| epoch 9 |  itr 141/351 | time 96.64045763015747[s] | loss 0.0003528160671703518\n",
      "| epoch 9 |  itr 161/351 | time 112.42877721786499[s] | loss 0.0003563839267008006\n",
      "| epoch 9 |  itr 181/351 | time 127.22688364982605[s] | loss 0.0011638923233840614\n",
      "| epoch 9 |  itr 201/351 | time 141.50042963027954[s] | loss 0.000314511536853388\n",
      "| epoch 9 |  itr 221/351 | time 156.19036650657654[s] | loss 0.000336730150738731\n",
      "| epoch 9 |  itr 241/351 | time 170.88086318969727[s] | loss 0.0003119730716571212\n",
      "| epoch 9 |  itr 261/351 | time 185.41303038597107[s] | loss 0.00029044404509477313\n",
      "| epoch 9 |  itr 281/351 | time 200.69992470741272[s] | loss 0.0011230177700053904\n",
      "| epoch 9 |  itr 301/351 | time 214.48140859603882[s] | loss 0.00030718900263309484\n",
      "| epoch 9 |  itr 321/351 | time 227.5505075454712[s] | loss 0.0002670775703154504\n",
      "| epoch 9 |  itr 341/351 | time 240.24085187911987[s] | loss 0.0003206596418749541\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc : 99.96000000000001\n",
      "| epoch 10 |  itr 1/351 | time 1.4657552242279053[s] | loss 0.0002428537467494607\n",
      "| epoch 10 |  itr 21/351 | time 30.601407051086426[s] | loss 0.0011006384680513292\n",
      "| epoch 10 |  itr 41/351 | time 55.62437057495117[s] | loss 0.0009585595282260329\n",
      "| epoch 10 |  itr 61/351 | time 67.63260293006897[s] | loss 0.0002455806138459593\n",
      "| epoch 10 |  itr 81/351 | time 79.54468870162964[s] | loss 0.00029914602986536926\n",
      "| epoch 10 |  itr 101/351 | time 91.49712419509888[s] | loss 0.00026128235156647856\n",
      "| epoch 10 |  itr 121/351 | time 104.9083480834961[s] | loss 0.00028297680430114266\n",
      "| epoch 10 |  itr 141/351 | time 118.17097544670105[s] | loss 0.00024715146166272463\n",
      "| epoch 10 |  itr 161/351 | time 131.35397696495056[s] | loss 0.00023492842912673948\n",
      "| epoch 10 |  itr 181/351 | time 144.57235479354858[s] | loss 0.007514124863082544\n",
      "| epoch 10 |  itr 201/351 | time 158.19066333770752[s] | loss 0.06942317530512812\n",
      "| epoch 10 |  itr 221/351 | time 171.45076084136963[s] | loss 0.006615995839238167\n",
      "| epoch 10 |  itr 241/351 | time 184.21099615097046[s] | loss 0.0029186056554317474\n",
      "| epoch 10 |  itr 261/351 | time 196.94017052650452[s] | loss 0.0018984643556177617\n",
      "| epoch 10 |  itr 281/351 | time 209.81155848503113[s] | loss 0.0019932867400348185\n",
      "| epoch 10 |  itr 301/351 | time 222.54086256027222[s] | loss 0.0013727291068062187\n",
      "| epoch 10 |  itr 321/351 | time 235.78480744361877[s] | loss 0.0012620267504826186\n",
      "| epoch 10 |  itr 341/351 | time 249.8266201019287[s] | loss 0.0011172249214723706\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc : 100.0\n"
     ]
    }
   ],
   "source": [
    "acc_list = []\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    trainer.fit(x_train, t_train, max_epoch=1, \n",
    "               batch_size=batch_size, max_grad=max_grad)\n",
    "    correct_num = 0\n",
    "    for i in range(len(x_test)):\n",
    "        question, correct = x_test[[i]], t_test[[i]]\n",
    "        verbose = i < 10\n",
    "        correct_num += eval_seq2seq(model, question, correct, \n",
    "                                   id_to_char, verbose, is_reverse=True)\n",
    "        \n",
    "    acc = float(correct_num) / len(x_test)\n",
    "    acc_list.append(acc)\n",
    "    \n",
    "    print(f'val acc : {acc*100}')\n",
    "    \n",
    "\n",
    "model.save_params()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T09:15:29.778084Z",
     "start_time": "2021-11-03T09:15:29.769741Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T09:16:05.719268Z",
     "start_time": "2021-11-03T09:16:05.582225Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(acc_list, '-o', label='attention')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-03T09:57:57.944571Z",
     "start_time": "2021-11-03T09:57:57.925709Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
